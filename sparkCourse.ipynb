{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Spark on Ubu\n",
    "https://sparkbyexamples.com/spark/spark-installation-on-linux-ubuntu/#:~:text=Apache%20Spark%20Installation%20on%20Ubuntu&text=Use%20wget%20command%20to%20download%20the%20Apache%20Spark%20to%20your%20Ubuntu%20server.&text=Once%20your%20download%20is%20complete,rename%20the%20folder%20to%20spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Spark\n",
    "\n",
    "spark scripts can bi in python, java or scala. scripts are run by the driver and distributed. spark can run on top of hadoop cluster. spark doesn't so anything untill its told to. it uses DAG to optimize tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig A.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is RDD\n",
    "Resilient Distributed Dataset is fundumentaly a dataset and it's an abstraction for a giant set of data. Need to know how to set up RDD objects and loading them up with big data sets and then calling various methods on the RDD object to distribute the processing of that data. The resilience and distribution is managed by the cluster manager. So RDD object transforms one set of data to another or perform actions to get results.\n",
    "\n",
    "The *SparkContext* gives us methods to create an RDD. \n",
    "\n",
    "## Transformations\n",
    "\n",
    "### Map\n",
    "Map allows you to take a set of data and transform it into another set of data, given a function that operates on the RDD. For ex. if I want to square all the numbers in an RDD, have a *map* that *points* to a *function* that multiplies everything in an RDD by itself.\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "rdd.map(lambda X: X*X) # square all numbers\n",
    "\n",
    "Ans: 1,4,8,16\n",
    "\n",
    "### Flatmap\n",
    "Has the capability to produce multiple values for every input value that you have in your original RDD. The original RDD might be larger or smaller than the resultant RDD.\n",
    "\n",
    "> With key/value data, use **mapValues()** and **flatMapValues()** if transformations do not effect the keys. These are more efficient.\n",
    "\n",
    "### Filter\n",
    "Get rid of information we do not need. Ex. Filter a lof file for errors only.\n",
    "\n",
    "### Distinct\n",
    "Unique values.\n",
    "\n",
    "### Sample\n",
    "Small junk from the original dataset.\n",
    "\n",
    "### Merge\n",
    "Union, intersection, subtract, cartesian\n",
    "\n",
    "## Actions\n",
    "\n",
    "### collect\n",
    "dump a values\n",
    "### count\n",
    "count all values\n",
    "### countByValue\n",
    "count breakdown by unique value\n",
    "### take\n",
    "sample few values\n",
    "### top\n",
    "sample few values\n",
    "### reduce\n",
    "combine different values for given key value\n",
    "\n",
    "**Nothing actually happenes in your driver program until an action is called!**\n",
    "\n",
    "## Spark can do special stuff with KEY/VALUE\n",
    "\n",
    "### reduceByKey\n",
    "combine values with the same key using some function. rdd.reduceByKey(lambda x, y: x+y) # adds them up\n",
    "\n",
    "### groupByKey\n",
    "group values with the same key\n",
    "\n",
    "### sortByKey\n",
    "sort RDD by key \n",
    "\n",
    "### keys(), values()\n",
    "create an RDD of just the keys or the values\n",
    "\n",
    "### join, rightOuterjoin, leftOuterjoin, cogroup, subtractByKey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig B.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig C.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "- Extend RDD to a DataFrame object\n",
    " - Contain row object\n",
    " - Can run SQL Queries\n",
    " - Can have a schema (leading to more efficient storage)\n",
    " - Read and write to JSON, Hive, Parquet, csv ...\n",
    " - Communicate with JDBC/ODBC, Tableau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate() # get or create cause we might already have a persistent session we want to reuse\n",
    "inputData = spark.read.json(\"DataFile\")\n",
    "inputData.createOrReplaceTempView(\"myStructuredStuff\") # to expose data, give the dataset a name to look like a database table\n",
    "myResultDataframe = spark.sql(\"select * from\")\n",
    "\n",
    "myResultDataframe.show() # show results and how many rows\n",
    "myResultDataframe.select(\"someFieldName\") # select column\n",
    "myResultDataframe.filter(myResultDataframe.select(\"someFieldName\") > 200) \n",
    "myResultDataframe.groupby(myResultDataframe.select(\"someFieldName\")).mean()\n",
    "myResultDataframe.rdd().map(mapperFunction) # for like mapreduce, change to rdd\n",
    "\n",
    "from pyspark.sql import functions as func\n",
    "func.explode() # similar to flatMap - explodes columns into rows\n",
    "func.split()\n",
    "func.lower()\n",
    "\n",
    "# passing columns as parameters\n",
    "func.split(inputDF.value, '\\\\W+')\n",
    "filter(wordsDF.word!=\"\")\n",
    "func.col(\"colName\") # Can also do to refer to a columns name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables\n",
    "Spark doesn't like joining dataframes as we do in SQL. So we load a lookup dictionary and broadcast it to the executor node. As spark distributes work load, broadcasting data means such lookups are available for all nodes in the cluster.         \n",
    "- Broadcast objects to the executors, such that they are always there whenever needed\n",
    "- Just use sc.broadcast() to ship of whatever you want\n",
    "- Then use .value() to get the object back\n",
    "- Use the broadcasted object however you want - map functions, UDFs..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "375aac7530e8c831bb65c3879b007db700405b020969fd2f9eab2e4f4141ee0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
